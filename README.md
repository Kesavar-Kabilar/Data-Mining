## Data Mining and Machine Learning Assignments: A Comprehensive Overview

### 1. Frequent Itemset Mining with Apriori Algorithm

I implemented the Apriori algorithm, a popular technique for discovering frequent itemsets and association rules in transactional data. This information is crucial for tasks like:

* **Market Basket Analysis:** By identifying frequently purchased items together, I can help retailers optimize product placement, create targeted promotions, and predict customer behavior.
* **Recommendation Systems:** By analyzing frequent itemsets, I can improve recommendation systems that suggest products to customers based on their past purchases or similar user behavior.
* **Understanding User Behavior Patterns:** I can uncover hidden relationships between products, providing valuable insights into user preferences and buying habits.

### 2. Sequential Pattern Mining with PrefixSpan Algorithm

I applied the PrefixSpan algorithm to discover frequent sequential patterns of words (phrases) within text data, such as reviews or product descriptions. Extracted sequences can be used for various purposes, including:

* **Sentiment Analysis:** By identifying frequently occurring positive or negative word sequences, I can assist in automated sentiment analysis of reviews or social media posts.
* **Product Recommendation Systems:** By understanding frequent sequences of words associated with product mentions, I can improve recommendation systems that suggest products that complement or logically follow from a user's expressed interest.
* **Understanding User Preferences:** Analyzing frequent word sequences helps me uncover user preferences and opinions based on the language they use.

### 3. Agglomerative Hierarchical Clustering for Geographic Data

I explored agglomerative hierarchical clustering, a method for grouping data points based on their similarity or proximity. I applied this technique to cluster geographical data points. This method is valuable for tasks such as:

* **Customer Segmentation:** By clustering customers based on their geographic location, I can help design targeted marketing campaigns and personalized offerings.
* **Urban Planning:** Identifying clusters of densely populated areas helps with urban planning and resource allocation.
* **Fraud Detection:** I can cluster transactions based on location to help detect potential fraudulent activities.

### 4. Evaluating Clustering Quality with Jaccard Similarity and NMI

I evaluated the quality of clustering results. I implemented two metrics: Jaccard Similarity and Normalized Mutual Information (NMI). These metrics help assess how well data points are grouped within a cluster and how distinct the clusters are from each other:

* **Jaccard Similarity:** Measures the similarity between two clusters by considering the ratio of shared data points between them.
* **Normalized Mutual Information (NMI):** Compares the information shared between the actual clustering and a random partitioning of data points.

### 5. Decision Tree Classification with Custom Maximum Depth

I built a decision tree classifier with a user-defined maximum depth. Decision trees learn by recursively splitting the data based on the most informative attributes. This approach offers several advantages, including:

* **Interpretability:** Decision trees are easy to interpret, allowing me to understand the key factors influencing the classification process.
* **Handling Categorical Data:** Decision trees natively handle categorical data, making them suitable for various real-world problems.
* **Efficient Classification:** Once built, decision trees can classify new data points quickly, making them ideal for applications that require real-time predictions.

### 6. Naive Bayes Classifier for Animal Classification

I implemented the Naive Bayes Classifier algorithm. This probabilistic classifier assumes independence between features (attributes) given the class label. Despite this simplifying assumption, Naive Bayes is still highly effective for tasks like:

* **Efficient Training:** Naive Bayes trains relatively quickly compared to other algorithms.
* **Effective Classification:** Despite the independence assumption, I found that Naive Bayes can achieve competitive performance in many classification tasks.